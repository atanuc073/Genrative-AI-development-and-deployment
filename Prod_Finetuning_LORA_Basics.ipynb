{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atanuc073/Genrative-AI-development-and-deployment/blob/main/Prod_Finetuning_LORA_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Creating the LoRA Layer"
      ],
      "metadata": {
        "id": "wKsErFq-1iR-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vzUBUCHLUTS"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LoRALayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
        "        super().__init__()\n",
        "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
        "        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
        "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.alpha * (x @ self.A @ self.B)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: LoRA applied to an existing linear layer"
      ],
      "metadata": {
        "id": "HlZ9jHc_1lck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearWithLoRA(nn.Module):\n",
        "\n",
        "    def __init__(self, linear, rank, alpha):\n",
        "        super().__init__()\n",
        "        self.linear = linear\n",
        "        self.lora = LoRALayer(\n",
        "            linear.in_features, linear.out_features, rank, alpha\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x) + self.lora(x)"
      ],
      "metadata": {
        "id": "nlNPDbtGLZhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.manual_seed(123)\n",
        "layer = nn.Linear(10, 2)\n",
        "x = torch.randn((1, 10))\n",
        "\n",
        "print(\"Original output:\", layer(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1q06uf25LbbW",
        "outputId": "afa5f160-39e5-42de-9817-ac4f16932cd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original output: tensor([[0.6639, 0.4487]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer_lora_1 = LinearWithLoRA(layer, rank=2, alpha=4)\n",
        "print(\"LoRA output:\", layer_lora_1(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5HnR8ybLn-M",
        "outputId": "cbfac8e2-0ddc-486b-feff-1aee1afddff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA output: tensor([[0.6639, 0.4487]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: A 3 layer model"
      ],
      "metadata": {
        "id": "SIl_sBMG1xqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = 768\n",
        "num_hidden_1 = 128\n",
        "num_hidden_2 = 256\n",
        "num_classes = 10\n",
        "\n",
        "class MultilayerPerceptron(nn.Module):\n",
        "    def __init__(self, num_features,\n",
        "        num_hidden_1, num_hidden_2, num_classes):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(num_features, num_hidden_1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(num_hidden_1, num_hidden_2),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(num_hidden_2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = MultilayerPerceptron(\n",
        "    num_features=num_features,\n",
        "    num_hidden_1=num_hidden_1,\n",
        "    num_hidden_2=num_hidden_2,\n",
        "    num_classes=num_classes\n",
        ")\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfBgpghuLx54",
        "outputId": "d33b583c-ebeb-4cb8-d234-08c4004211b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultilayerPerceptron(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=256, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: 3 layer model with LoRA"
      ],
      "metadata": {
        "id": "3Dti2y1V13ZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[0] = LinearWithLoRA(model.layers[0], rank=4, alpha=8)\n",
        "model.layers[2] = LinearWithLoRA(model.layers[2], rank=4, alpha=8)\n",
        "model.layers[4] = LinearWithLoRA(model.layers[4], rank=4, alpha=8)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fP686Ks5MRad",
        "outputId": "ac6d02f5-a31b-429a-d485-1b3b9c85bb3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultilayerPerceptron(\n",
            "  (layers): Sequential(\n",
            "    (0): LinearWithLoRA(\n",
            "      (linear): Linear(in_features=768, out_features=128, bias=True)\n",
            "      (lora): LoRALayer()\n",
            "    )\n",
            "    (1): ReLU()\n",
            "    (2): LinearWithLoRA(\n",
            "      (linear): Linear(in_features=128, out_features=256, bias=True)\n",
            "      (lora): LoRALayer()\n",
            "    )\n",
            "    (3): ReLU()\n",
            "    (4): LinearWithLoRA(\n",
            "      (linear): Linear(in_features=256, out_features=10, bias=True)\n",
            "      (lora): LoRALayer()\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze_linear_layers(model):\n",
        "    for child in model.children():\n",
        "        if isinstance(child, nn.Linear):\n",
        "            for param in child.parameters():\n",
        "                param.requires_grad = False\n",
        "        else:\n",
        "            # Recursively freeze linear layers in children modules\n",
        "            freeze_linear_layers(child)\n",
        "\n",
        "freeze_linear_layers(model)\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name}: {param.requires_grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTw0YFlsMaOH",
        "outputId": "fde6eb40-dee9-4c40-91b5-ab0c8b7c831d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers.0.linear.weight: False\n",
            "layers.0.linear.bias: False\n",
            "layers.0.lora.A: True\n",
            "layers.0.lora.B: True\n",
            "layers.2.linear.weight: False\n",
            "layers.2.linear.bias: False\n",
            "layers.2.lora.A: True\n",
            "layers.2.lora.B: True\n",
            "layers.4.linear.weight: False\n",
            "layers.4.linear.bias: False\n",
            "layers.4.lora.A: True\n",
            "layers.4.lora.B: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You’ll need scikit-learn available for the dataset + standardization\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "9aJu_FT7zeiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Dataset - 768-dim, 10 classes"
      ],
      "metadata": {
        "id": "5lgV9wV_0MgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "num_features = 768\n",
        "num_classes  = 10\n",
        "\n",
        "N_SAMPLES = 12000  # tweak as desired\n",
        "VAL_SPLIT = 0.10\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_samples=N_SAMPLES,\n",
        "    n_features=num_features,\n",
        "    n_informative=64,\n",
        "    n_redundant=16,\n",
        "    n_repeated=0,\n",
        "    n_classes=num_classes,\n",
        "    n_clusters_per_class=2,\n",
        "    class_sep=2.0,\n",
        "    flip_y=0.01,\n",
        "    random_state=0,\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X).astype(np.float32)\n",
        "y = y.astype(np.int64)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=VAL_SPLIT, stratify=y, random_state=0\n",
        ")\n",
        "\n",
        "train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "val_ds   = TensorDataset(torch.from_numpy(X_val),   torch.from_numpy(y_val))\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=256, shuffle=False)\n",
        "\n",
        "len(train_ds), len(val_ds), X_train.shape, y_train[:5]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aNEz6HtzfXQ",
        "outputId": "35874f55-24b5-40b5-af11-9534b4972ee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10800, 1200, (10800, 768), array([5, 7, 8, 4, 6]))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Build comparable models (LoRA student vs Full student)"
      ],
      "metadata": {
        "id": "OG4X6K1O0W4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "# Uses MultilayerPerceptron, LinearWithLoRA, LoRALayer definitions\n",
        "\n",
        "# Dims (must match dataset)\n",
        "num_hidden_1 = 128\n",
        "num_hidden_2 = 256\n",
        "\n",
        "# fresh base for fair init\n",
        "base = MultilayerPerceptron(\n",
        "    num_features=num_features,\n",
        "    num_hidden_1=num_hidden_1,\n",
        "    num_hidden_2=num_hidden_2,\n",
        "    num_classes=num_classes\n",
        ")\n",
        "base_sd = deepcopy(base.state_dict())\n",
        "\n",
        "# --- LoRA student ---\n",
        "lora_student = MultilayerPerceptron(\n",
        "    num_features=num_features,\n",
        "    num_hidden_1=num_hidden_1,\n",
        "    num_hidden_2=num_hidden_2,\n",
        "    num_classes=num_classes\n",
        ")\n",
        "# wrap linears with LoRA\n",
        "lora_student.layers[0] = LinearWithLoRA(lora_student.layers[0], rank=4, alpha=8)\n",
        "lora_student.layers[2] = LinearWithLoRA(lora_student.layers[2], rank=4, alpha=8)\n",
        "lora_student.layers[4] = LinearWithLoRA(lora_student.layers[4], rank=4, alpha=8)\n",
        "\n",
        "# load identical base weights\n",
        "lora_student.load_state_dict(base_sd, strict=False)\n",
        "\n",
        "# freeze base linear weights; keep LoRA A/B trainable\n",
        "def freeze_linear_layers(module: nn.Module):\n",
        "    for child in module.children():\n",
        "        if isinstance(child, nn.Linear):\n",
        "            for p in child.parameters():\n",
        "                p.requires_grad = False\n",
        "        else:\n",
        "            freeze_linear_layers(child)\n",
        "\n",
        "freeze_linear_layers(lora_student)\n",
        "\n",
        "# --- Full student ---\n",
        "full_student = MultilayerPerceptron(\n",
        "    num_features=num_features,\n",
        "    num_hidden_1=num_hidden_1,\n",
        "    num_hidden_2=num_hidden_2,\n",
        "    num_classes=num_classes\n",
        ")\n",
        "full_student.load_state_dict(base_sd, strict=False)\n",
        "\n",
        "def count_trainable_params(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"LoRA trainables: {count_trainable_params(lora_student):,}\")\n",
        "print(f\"Full  trainables: {count_trainable_params(full_student):,}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzMThdx9znCF",
        "outputId": "f43c528c-54d6-432f-f664-ba09c6fe8dd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA trainables: 6,184\n",
            "Full  trainables: 134,026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Optimizers"
      ],
      "metadata": {
        "id": "GdN9HKaE0rTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "opt_lora = torch.optim.AdamW(\n",
        "    (p for p in lora_student.parameters() if p.requires_grad),\n",
        "    lr=2e-3, weight_decay=0.0\n",
        ")\n",
        "opt_full = torch.optim.AdamW(\n",
        "    full_student.parameters(),\n",
        "    lr=1e-3, weight_decay=0.0\n",
        ")\n"
      ],
      "metadata": {
        "id": "6GpPPlVB0sYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Training loop definition"
      ],
      "metadata": {
        "id": "ubJzEy_t0vQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_epoch(model, loader, optimizer=None, criterion=None, device=device):\n",
        "    train = optimizer is not None\n",
        "    model.train(mode=train)\n",
        "    total_loss, total_correct, total_n = 0.0, 0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        if train:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                logits = model(xb)\n",
        "                loss = criterion(logits, yb)\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "        preds = logits.argmax(dim=-1)\n",
        "        total_correct += (preds == yb).sum().item()\n",
        "        total_n += xb.size(0)\n",
        "    return total_loss / total_n, total_correct / total_n\n"
      ],
      "metadata": {
        "id": "n_8Btx9V0yYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Joint training loop"
      ],
      "metadata": {
        "id": "46sVAod900Ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "EPOCHS = 1000  # you can lower to 100–200 for quick runs\n",
        "\n",
        "parity_epoch_val_loss = None\n",
        "parity_epoch_val_acc  = None\n",
        "\n",
        "t0 = time.time()\n",
        "for ep in range(1, EPOCHS + 1):\n",
        "    tr_lora = run_epoch(lora_student, train_loader, optimizer=opt_lora, criterion=criterion)\n",
        "    tr_full = run_epoch(full_student, train_loader, optimizer=opt_full, criterion=criterion)\n",
        "\n",
        "    va_lora = run_epoch(lora_student, val_loader, optimizer=None, criterion=criterion)\n",
        "    va_full = run_epoch(full_student, val_loader, optimizer=None, criterion=criterion)\n",
        "\n",
        "    (tr_loss_lora, tr_acc_lora), (tr_loss_full, tr_acc_full) = tr_lora, tr_full\n",
        "    (va_loss_lora, va_acc_lora), (va_loss_full, va_acc_full) = va_lora, va_full\n",
        "\n",
        "    if ep % 5 == 0 or ep == 1:\n",
        "        print(f\"[Ep {ep:04d}] \"\n",
        "              f\"LoRA: train {tr_loss_lora:.4f}/{tr_acc_lora:.3f} | val {va_loss_lora:.4f}/{va_acc_lora:.3f}   \"\n",
        "              f\"Full: train {tr_loss_full:.4f}/{tr_acc_full:.3f} | val {va_loss_full:.4f}/{va_acc_full:.3f}\")\n",
        "\n",
        "    if parity_epoch_val_loss is None and va_loss_lora <= va_loss_full:\n",
        "        parity_epoch_val_loss = ep\n",
        "    if parity_epoch_val_acc is None and va_acc_lora >= va_acc_full:\n",
        "        parity_epoch_val_acc = ep\n",
        "\n",
        "total_time = time.time() - t0\n",
        "\n",
        "print(\"\\n=== Parity summary (validation) ===\")\n",
        "print(\"Val loss parity:\", parity_epoch_val_loss if parity_epoch_val_loss is not None else \"Not reached\")\n",
        "print(\"Val acc  parity:\", parity_epoch_val_acc  if parity_epoch_val_acc  is not None else \"Not reached\")\n",
        "print(f\"Total time: {total_time:.2f}s for {ep} epoch(s)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OChDXaL004DF",
        "outputId": "8ea9f68b-32c5-458a-b0ec-8cc65f4a732d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Ep 0001] LoRA: train 2.3323/0.118 | val 2.3046/0.123   Full: train 1.7489/0.437 | val 0.9704/0.677\n",
            "[Ep 0005] LoRA: train 2.1728/0.195 | val 2.1780/0.202   Full: train 0.0326/0.998 | val 1.0874/0.722\n",
            "[Ep 0010] LoRA: train 1.7629/0.354 | val 1.8206/0.339   Full: train 0.0021/1.000 | val 1.2437/0.730\n",
            "[Ep 0015] LoRA: train 1.4709/0.471 | val 1.6153/0.415   Full: train 0.0008/1.000 | val 1.3399/0.728\n",
            "[Ep 0020] LoRA: train 1.3128/0.528 | val 1.5185/0.466   Full: train 0.0004/1.000 | val 1.4141/0.732\n",
            "[Ep 0025] LoRA: train 1.2322/0.555 | val 1.4932/0.481   Full: train 0.0003/1.000 | val 1.4761/0.728\n",
            "[Ep 0030] LoRA: train 1.2050/0.565 | val 1.4704/0.486   Full: train 0.0002/1.000 | val 1.5330/0.728\n",
            "[Ep 0035] LoRA: train 1.1520/0.583 | val 1.4418/0.495   Full: train 0.0001/1.000 | val 1.5840/0.728\n",
            "[Ep 0040] LoRA: train 1.1269/0.595 | val 1.4856/0.478   Full: train 0.0001/1.000 | val 1.6312/0.727\n",
            "[Ep 0045] LoRA: train 1.1263/0.597 | val 1.4874/0.489   Full: train 0.0001/1.000 | val 1.6732/0.727\n",
            "[Ep 0050] LoRA: train 1.1058/0.599 | val 1.5077/0.482   Full: train 0.0000/1.000 | val 1.7153/0.726\n",
            "[Ep 0055] LoRA: train 1.1031/0.603 | val 1.5216/0.484   Full: train 0.0000/1.000 | val 1.7547/0.727\n",
            "[Ep 0060] LoRA: train 1.0855/0.606 | val 1.5199/0.482   Full: train 0.0000/1.000 | val 1.7926/0.727\n",
            "[Ep 0065] LoRA: train 1.0852/0.611 | val 1.5095/0.486   Full: train 0.0000/1.000 | val 1.8313/0.727\n",
            "[Ep 0070] LoRA: train 1.0745/0.614 | val 1.5582/0.488   Full: train 0.0000/1.000 | val 1.8671/0.726\n",
            "[Ep 0075] LoRA: train 1.0801/0.612 | val 1.5298/0.473   Full: train 0.0000/1.000 | val 1.9033/0.727\n",
            "[Ep 0080] LoRA: train 1.0748/0.616 | val 1.5372/0.482   Full: train 0.0000/1.000 | val 1.9384/0.727\n",
            "[Ep 0085] LoRA: train 1.0699/0.618 | val 1.5355/0.480   Full: train 0.0000/1.000 | val 1.9741/0.727\n",
            "[Ep 0090] LoRA: train 1.0622/0.622 | val 1.5375/0.496   Full: train 0.0000/1.000 | val 2.0092/0.727\n",
            "[Ep 0095] LoRA: train 1.0671/0.620 | val 1.5405/0.481   Full: train 0.0000/1.000 | val 2.0441/0.728\n",
            "[Ep 0100] LoRA: train 1.0658/0.619 | val 1.5830/0.483   Full: train 0.0000/1.000 | val 2.0782/0.729\n",
            "[Ep 0105] LoRA: train 1.0568/0.627 | val 1.5655/0.469   Full: train 0.0000/1.000 | val 2.1121/0.729\n",
            "[Ep 0110] LoRA: train 1.0554/0.624 | val 1.5695/0.486   Full: train 0.0000/1.000 | val 2.1463/0.730\n",
            "[Ep 0115] LoRA: train 1.0532/0.626 | val 1.5654/0.484   Full: train 0.0000/1.000 | val 2.1801/0.729\n",
            "[Ep 0120] LoRA: train 1.0584/0.623 | val 1.5809/0.499   Full: train 0.0000/1.000 | val 2.2140/0.729\n",
            "[Ep 0125] LoRA: train 1.0555/0.625 | val 1.5921/0.490   Full: train 0.0000/1.000 | val 2.2478/0.729\n",
            "[Ep 0130] LoRA: train 1.0581/0.623 | val 1.5891/0.476   Full: train 0.0000/1.000 | val 2.2813/0.730\n",
            "[Ep 0135] LoRA: train 1.0367/0.632 | val 1.5806/0.489   Full: train 0.0000/1.000 | val 2.3144/0.730\n",
            "[Ep 0140] LoRA: train 1.0376/0.633 | val 1.5978/0.472   Full: train 0.0000/1.000 | val 2.3475/0.730\n",
            "[Ep 0145] LoRA: train 1.0365/0.628 | val 1.6174/0.476   Full: train 0.0000/1.000 | val 2.3803/0.731\n",
            "[Ep 0150] LoRA: train 1.0442/0.634 | val 1.6237/0.486   Full: train 0.0000/1.000 | val 2.4125/0.730\n",
            "[Ep 0155] LoRA: train 1.0322/0.630 | val 1.6163/0.491   Full: train 0.0000/1.000 | val 2.4440/0.731\n",
            "[Ep 0160] LoRA: train 1.0360/0.631 | val 1.6244/0.467   Full: train 0.0000/1.000 | val 2.4752/0.732\n",
            "[Ep 0165] LoRA: train 1.0323/0.632 | val 1.6384/0.477   Full: train 0.0000/1.000 | val 2.5059/0.732\n",
            "[Ep 0170] LoRA: train 1.0435/0.631 | val 1.6796/0.480   Full: train 0.0000/1.000 | val 2.5347/0.733\n",
            "[Ep 0175] LoRA: train 1.0129/0.635 | val 1.6530/0.472   Full: train 0.0000/1.000 | val 2.5627/0.732\n",
            "[Ep 0180] LoRA: train 1.0177/0.636 | val 1.6893/0.466   Full: train 0.0000/1.000 | val 2.5876/0.733\n",
            "[Ep 0185] LoRA: train 1.0077/0.639 | val 1.6937/0.467   Full: train 0.0000/1.000 | val 2.6100/0.731\n",
            "[Ep 0190] LoRA: train 1.0155/0.639 | val 1.6994/0.481   Full: train 0.0000/1.000 | val 2.6271/0.732\n",
            "[Ep 0195] LoRA: train 1.0045/0.640 | val 1.7039/0.478   Full: train 0.0000/1.000 | val 2.6362/0.733\n",
            "[Ep 0200] LoRA: train 1.0178/0.637 | val 1.7189/0.480   Full: train 0.0000/1.000 | val 2.6362/0.733\n",
            "[Ep 0205] LoRA: train 0.9993/0.644 | val 1.6842/0.458   Full: train 0.0000/1.000 | val 2.6305/0.733\n",
            "[Ep 0210] LoRA: train 0.9999/0.642 | val 1.7517/0.472   Full: train 0.0000/1.000 | val 2.6251/0.733\n",
            "[Ep 0215] LoRA: train 1.0028/0.643 | val 1.8004/0.466   Full: train 0.0000/1.000 | val 2.6317/0.731\n",
            "[Ep 0220] LoRA: train 1.0154/0.637 | val 1.7402/0.465   Full: train 0.0000/1.000 | val 2.6686/0.733\n",
            "[Ep 0225] LoRA: train 0.9930/0.644 | val 1.7494/0.457   Full: train 0.0000/1.000 | val 2.7560/0.737\n",
            "[Ep 0230] LoRA: train 0.9927/0.641 | val 1.7879/0.469   Full: train 0.0000/1.000 | val 2.8704/0.738\n",
            "[Ep 0235] LoRA: train 0.9883/0.643 | val 1.7902/0.455   Full: train 0.0000/1.000 | val 3.0178/0.741\n",
            "[Ep 0240] LoRA: train 1.0020/0.639 | val 1.8162/0.467   Full: train 0.0000/1.000 | val 3.1981/0.737\n",
            "[Ep 0245] LoRA: train 1.0034/0.638 | val 1.7896/0.466   Full: train 0.0000/1.000 | val 3.4102/0.738\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2338280769.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtr_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_student\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mva_lora\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlora_student\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mva_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_student\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-352712880.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}